% vim: set fenc=utf-8 ft=latex encoding=utf-8
% -*- mode: latex; coding: UTF-8; -*-
%!TEX root = knowledge-curation.tex
\section{Methodology}
\label{cha:methodology}

The main goal of this work is to empirically compare how knowledge, specifically knowledge in question-and-answer (Q\&A) form, is sought, shared, and curated in both the \RH mailing list and the R tag in \SO. We used a qualitative \textit{exploratory case study} methodology~\cite{Creswell2009,Runeson2012} to answer the following research questions:
    \begin{enumerate}[label=\bfseries{RQ-\arabic*.},itemsep=3pt, topsep=2pt, leftmargin=3em, parsep=0pt]
        \item \rqa
        \item \rqb
        \item \rqc
     \end{enumerate}

%We performed a qualitative case study~\cite{Creswell2009,Runeson2012} of the knowledge that flows through both \RH and \SO.
%This research method is applicable when a concept or phenomenon requires more understanding, with little pre-existing research~\cite{Creswell2009}.

This study employed two research methods over two phases: we \textit{mined archival data} and then conducted a \textit{qualitative survey}. In the first phase, we randomly sampled questions in both channels to characterize the types of discussions that occur. We sampled 400 questions in each channel---this sample guaranteed both saturation and a reliability of 95\% $\pm$ 5\%. \alexey{How? more details on saturation.} In the second phase, we surveyed the members of the R community to validate our interpretation of the results from the previous phase. %obtain further additional insights and to verify our findings.

\subsection{Phase I: Mining Archival Data} 
\label{sec:studyDesign}
We used the public archives of both	the R-help mailing list and Stack Overflow. The R-help mailing list archive starts in 1997, while the archives for Stack Overflow start in 2008 (when it was created).
To make both data sets comparable, we analyzed both datasets from September 2008 until December 2013, a period of time that both channels were available.
For Stack Overflow, we obtained a data dump file available on their Website---Stack Exchange releases a new data dump from all their Websites every three months\footnote{\url{http://stackexchange.com/sites}}. For the R-help mailing list data, we retrieved the archives available as MBOX files from the R-help Website.

%To make the data comparable againt the Stack Overflow dat set, we transformed the email addresses to MD5 hashes, and changed the time zone of the mailing list messages (UTC+2) to the time zone used by Stack Overflow (UTC).
    
In order to answer RQ-3, we needed the ability to compare email addresses between both channels. However, Stack Exchange has stopped providing information about email addresses and the last Stack Exchange dump file to contain email addresses as MD5 hashes was released in September 2013.
To complete the email addresses from October 2013 to December 2013, we used the data dump file from September 2014, but updated the \texttt{users} table with the hashes taken from the September 2013 dump file for those \texttt{ID}s that were identical in both data sets.
However, if a user in the 2013 data file did not exist in the 2014 data, we did not count them.
From \SO, we retrieved all R-related data by selecting only messages with the R tag (\texttt{r}) and its synonyms\footnote{\url{http://stackoverflow.com/tags/r/synonyms}} (\texttt{rstats} and \texttt{r-language}).

To determine users who were active in both channels, we compared the MD5 hashes of the R-help participants to the MD5 hashes of the \SO participants. Given the limitations of the \SO data, we did not perform any unification of email addresses and considered that every email address belongs to an independent individual. In total, we found 1,421 different users (email addresses) on both media channels.

    % Although MD5 hashes are not \textit{collision resistant} and could possibly lead to false positives, it is unlikely that two different email addresses share a MD5 hash.

We used two different software tools to prepare the data:
    \begin{enumerate*}[label=(\arabic*)]
    \item To process the Stack Overflow data, we used a modified version of Sam Saffron's application, So-Slow\footnote{\url{https://github.com/SamSaffron/So-Slow}}.
    \item To process the R-help mailing list data, we wrote our own mail mining application\footnote{Our tool is available at
            \url{https://github.com/cagomezt/GTMail}}. To ensure accurate results when processing the R-help mailing list, we followed a series of recommendations proposed by Bettenburg \textit{et al}.~\cite{Bettenburg2009}.
    \end{enumerate*}
Table~\ref{table:data} depicts a summary of the data used for this study. Unsurprisingly, the R-help mailing list has more questions, answers, and users as it contains approximately ten years of additional data.
Note that only Stack Overflow's data contains ``comments'' information.

	\begin{table}[!htb]
	  \centering
      \caption{Raw data collected for each channel.}
      \begin{small}
        \begin{tabular}{lrr}
	        \toprule
	        Type          &  R-help & Stack Overflow \\
	        \midrule
	        Questions     & 101,931 &  67,393 \\
	        Answers       & 213,366 &  99,620 \\
	        Comments      &       - & 286,124 \\
	        Users         &  39,150 &  26,324 \\
	        \bottomrule
        \end{tabular}
      \end{small}
	  \label{table:data}
	\end{table}





\subsubsection{Data Analysis Process}
\label{sec:dap}

We followed an inductive approach~\cite{Runeson2012} to analyze the data from Stack Overflow and the R-help mailing list. Through this process, we iteratively switched between \textit{data gathering} and \textit{data analysis}, until we reached saturation. To reduce the risk of bias~\cite{Runeson2012}, two researchers conducted the analysis, both computer scientists with a background in qualitative data analysis. To answer RQ1 and RQ2, we selected 400 random threads from each channel. To answer RQ3, we focused on questions with identical subjects that were posted to both channels by the same author---we found 79 such threads and analyzed them all.
    
    \begin{figure*}[htbp]
    	\centering
    	\includegraphics[width=.95\textwidth]{Figures/CodingExample}
    	\caption{Example of data coding. Each row is a threaded message. Questions, comments, and answers are identified with the number on the first column. Columns in yellow contain the codification\cassie{Do you mean code?} for each message type. The last two columns contain the memos and the URL.}
    	\label{fig:CodingExample}
%\vspace{-3mm}
    \end{figure*}

We used \textbf{memoing}, \textbf{affinity diagrams}, and a \textbf{code book} to support the data analysis process. We wrote reflective memos in a spreadsheet next to the applicable codes (see example in Fig.~\ref{fig:CodingExample}). These memos were used to create the codes and hypotheses about the relationships between concepts. We coded in multiple sessions, which allowed us to refine the code book definitions. Each entry is associated with a title, a formal definition, an example, and notes from the researcher. %The final version of the code book is detailed in section~\ref{cha:findings}.
For inter-rater reliability, we used the Cohen Kappa \textbf{inter-rater agreement} coefficient~\cite{Stemler2004}. Although it is suggested to aim for coefficient values above 0.6 to obtain substantial results~\cite{Landis1977}, based on our previous experience with this method~\cite{Gomez2013}, we aimed for 0.8 or above. We used this coefficient after each coding session as a way to trigger discussion.



	% \begin{description}[itemsep=2pt, topsep=0pt, leftmargin=1em, parsep=0pt]
	% 	\item[Memoing] is the act of taking notes (coding) on what the researcher is learning from the data during the analysis~\cite{Groenewald2008}.
 %        We wrote reflective memos in a spreadsheet next to the applicable codes (see figure \ref{fig:CodingExample}).
 %        These memos were used to create the codes, and hypotheses about the relationships between concepts.

	% 	\item[Affinity diagrams] allows to organize ideas and data into groups, and to find the relationships between concepts~\cite{Scupin1997}.
	% 	We used them to discuss new insights, and while defining categories and relationships between them.

	% 	\item[Inter-rater agreement \textit{Cohen Kappa}] is a coefficient used to measure the agreement between two coders who classify items into mutually exclusive categories~\cite{Stemler2004}.
	% 	Ladis and Koch suggest that values above 0.60 or 60\% to obtain substantial results~\cite{Landis1977}.
	% 	In a previous study~\cite{Gomez2013}, we used the same coefficient to measure agreement between coders.
	% 	Based on this experience, we set a value above 0.80 or 80\% as the minimum to obtain substantial results.
	% 	We used this coefficient after each coding session as a way to trigger discussion.

	% 	\item[Code book] is a book that contains the definitions of the codes that researchers look during the data analysis~\cite{MacQueen1998}.
	% 	We coded in multiple sessions, which allowed us to refine the definitions.
	% 	Each entry is associated with a title, a formal definition, an example, and space for notes from the researcher.
	% 	The final version of the code book is detailed in section~\ref{cha:findings}.
	% \end{description}

The analysis process required an \textit{understanding of the context} surrounding each message. The process consisted of: (1) gathering the required information from each channel (i.e., the message analyzed, the relevant thread), and (2) mapping between messages from each channel to a specific knowledge type (see Section~\ref{cha:findings-types}). The mapping is necessary as each channel contains a different data structure.

For mapping messages between both channels, we defined the following:

	\begin{description}[itemsep=1pt, topsep=2pt, leftmargin=1em, parsep=0pt]
		\item[Question:] the message is the first in the thread and it contains the main question.
		\item[Answer:] the message provides a solution to the main question of the thread.
	 	\item[Update:] the message requests a modification to a question (or answer) made by the author of said question (or answer).
		\item[Comment:] the message offers a clarification to a specific part of the question or answer.
		\item[Flag:] the message requests attention from the moderator (e.g., repeated questions, spam, or rude behavior).
	\end{description}

%	The data set was capped at 400 threads for each channel when we deemed our observations as being saturated.

\subsection{Phase II: Qualitative Survey} 

\alexey{German mentioned that we indeed use this as quotes later on. It would help to mention what RQ it applies to, and the goal of the survey (I guess we did it to validate our interpretation of the findings from the previous phase). Also, how long did we run the survey for? Do we have any demographics that might be relevant?}

We conducted a survey\footnote{A copy of the survey is available at \url{http://goo.gl/mxmH5J}} with members of the R community in order to validate the our interpretation of the results from Phase I. %with the purpose of obtaining additional insights on the findings.
To test and refine the questions, format and tone, we performed two survey pilots. 
We promoted our survey on Twitter, Reddit, the R-help mailing list, and Meta Stack Exchange, to reach users of both channels and minimize the selection bias. However, our participation invitation on Stack Exchange the was deemed out of topic and was deleted a few minutes later. In total, we received 26 valid responses.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "knowledge-curation.tex"
%%% End:
